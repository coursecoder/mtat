# Prompt Iteration Lab

## Overview

This lab gives you hands-on practice with the prompt engineering framework from the previous modules. You will choose a real task from your own work, write an initial prompt, evaluate the output, and iterate at least twice. The goal is not a perfect prompt — it is a documented iteration process that you can apply to any task going forward.

Plan for approximately 30 minutes.

## Before You Begin

You will need:
- Access to an LLM (Claude, ChatGPT, or equivalent)
- A real task or recurring writing need from your own workflow
- This worksheet (or a blank doc to record your work)

## Step 1: Choose Your Task

Select a task that meets these criteria:
- It is something you do regularly (so you know what "good" looks like)
- The output can be evaluated in under 5 minutes
- It involves producing text: an email, a summary, a list, a policy section, a status update, documentation, or similar

Write your task in one sentence:

> **My task:** _______________________________________________

## Step 2: Write Your First Prompt (Unstructured)

Write the first prompt that comes to mind — the one you would type if you were in a hurry. Do not over-think it. This is your baseline.

Submit it to the model and save the output.

> **Prompt v1:** _______________________________________________

> **Output v1:** [paste or summarize the output here]

## Step 3: Diagnose the Output

Evaluate the output against your actual intent. For each element, note what was missing or underspecified:

| Element | Present in v1? | What was missing or unclear? |
|---|---|---|
| Context | Yes / Partial / No | |
| Task specification | Yes / Partial / No | |
| Constraints | Yes / Partial / No | |

**Overall assessment:** What is the most significant gap between what you received and what you needed?

> _______________________________________________

## Step 4: Write Your Second Prompt (Structured)

Using your diagnosis, rewrite the prompt. Add or tighten each element where you identified a gap. Be explicit.

Submit it to the model and save the output.

> **Prompt v2:** _______________________________________________

> **Output v2:** [paste or summarize the output here]

## Step 5: Compare and Iterate Again

Compare v1 and v2 outputs:
- What specifically improved?
- What still needs work?

Write a third prompt that addresses any remaining gaps.

> **Prompt v3:** _______________________________________________

> **Output v3:** [paste or summarize the output here]

## Step 6: Reflect

Answer these questions in 2–3 sentences each:

1. **What changed most between your first and final prompt?**

2. **What element (context, task specification, or constraints) had the most impact when you added or improved it? Why?**

3. **How would you structure this prompt if you were handing it to a teammate to use repeatedly?**

## Sharing Your Work

If you are completing this lab in a group setting, pair with one other participant and exchange your final prompts. Try running each other's v3 prompts and compare outputs. Discuss:

- Does the prompt produce the same quality output for someone who did not write it?
- What additional constraints would make it more portable?

## What to Take Forward

A prompt you have iterated 2–3 times is a reusable asset. Save your best prompts somewhere accessible — a shared doc, a prompt library, a Notion page. The next time you face the same task, start from your iterated version, not from scratch.
